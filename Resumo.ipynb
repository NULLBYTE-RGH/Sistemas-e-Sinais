{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CRISP-DM, Saiba descrever e identificar as 6 fases do CRISP-DM\n",
    "\n",
    "O CRISP-DM (Cross-Industry Standard Process for Data Mining) é um modelo amplamente utilizado para orientar projetos de mineração de dados e análise de dados. Ele fornece uma estrutura sistemática para abordar e gerenciar esses projetos, dividindo-os em seis fases distintas. Vou descrever cada uma delas para você:\n",
    "\n",
    "1. Compreensão do Negócio: Nesta fase inicial, é crucial entender completamente os objetivos e requisitos do negócio. Isso envolve reunir informações sobre o contexto, estabelecer metas claras e formular perguntas de negócio específicas que o projeto deve responder. Além disso, identificam-se os recursos disponíveis, as restrições e os riscos associados ao projeto.\n",
    "\n",
    "2. Compreensão dos Dados: Aqui, o foco é adquirir uma visão aprofundada dos dados disponíveis. Isso inclui coletar os dados relevantes, explorá-los e realizar análises preliminares para identificar sua qualidade e sua adequação aos objetivos do projeto. Também é importante documentar as características dos dados, identificar possíveis problemas e definir as transformações necessárias para prepará-los para as etapas subsequentes.\n",
    "\n",
    "3. Preparação dos Dados: Esta fase envolve a preparação dos dados para a modelagem. Isso inclui limpar os dados, tratar valores ausentes, selecionar as variáveis mais relevantes, criar novas variáveis derivadas e realizar transformações adicionais. O objetivo é obter um conjunto de dados pronto para ser usado nas etapas de modelagem.\n",
    "\n",
    "4. Modelagem: Nesta fase, são aplicadas técnicas de modelagem e mineração de dados para explorar os dados e desenvolver modelos. Diferentes algoritmos e abordagens são testados e avaliados para identificar o modelo que melhor atende aos objetivos do projeto. Os modelos desenvolvidos são avaliados com base em critérios de desempenho e refinados conforme necessário.\n",
    "\n",
    "5. Avaliação: Aqui, os modelos criados são avaliados de forma mais detalhada para determinar sua eficácia em atender aos requisitos do projeto. Isso envolve a análise de seus resultados e interpretação de suas capacidades e limitações. Também podem ser realizadas validações externas para verificar se o modelo é geral o suficiente para ser aplicado a novos dados.\n",
    "\n",
    "6. Implantação: Nesta última fase, os resultados do projeto são apresentados e implantados na organização. Isso pode envolver a criação de relatórios, visualizações de dados ou sistemas automatizados de tomada de decisão baseados em modelos. Também é importante desenvolver um plano de monitoramento e acompanhamento contínuo para garantir que os modelos implantados estejam funcionando corretamente e gerando valor ao negócio.\n",
    "\n",
    "Essas seis fases do CRISP-DM são iterativas, o que significa que, ao longo do processo, é comum revisar e refinar etapas anteriores à medida que novos insights são descobertos ou requisitos do negócio evoluem. Isso garante uma abordagem flexível e adaptativa para o desenvolvimento de projetos de mineração de dados bem-sucedidos."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Science X Machine Learning X IA\n",
    "\n",
    "Data Science, Machine Learning e Inteligência Artificial (IA) são termos relacionados, mas têm significados distintos. Vou explicar as diferenças entre eles:\n",
    "\n",
    "Data Science (Ciência de Dados): Data Science é uma disciplina que envolve a extração de insights e conhecimentos valiosos a partir de grandes conjuntos de dados. Os cientistas de dados utilizam habilidades e técnicas de estatística, programação, matemática e domínio do negócio para coletar, limpar, processar e analisar dados. O objetivo da Ciência de Dados é descobrir padrões, tendências e relações nos dados, a fim de obter insights que possam ser usados para tomar decisões informadas e resolver problemas complexos.\n",
    "\n",
    "Machine Learning (Aprendizado de Máquina): Machine Learning é uma subárea da Inteligência Artificial que se concentra no desenvolvimento de algoritmos e modelos que permitem que os computadores aprendam a partir de dados e executem tarefas específicas sem serem explicitamente programados. Em vez de seguir um conjunto de instruções rígidas, os modelos de Machine Learning são treinados com dados para reconhecer padrões e tomar decisões ou fazer previsões com base nesses padrões. O Machine Learning é usado em uma ampla gama de aplicações, como classificação de dados, detecção de fraudes, recomendação de produtos e reconhecimento de voz.\n",
    "\n",
    "Inteligência Artificial (IA): A Inteligência Artificial é um campo mais amplo que se concentra no desenvolvimento de sistemas ou agentes capazes de realizar tarefas que normalmente exigiriam inteligência humana. Ela engloba várias técnicas e abordagens, incluindo Machine Learning, processamento de linguagem natural, visão computacional e raciocínio baseado em regras. O objetivo da IA é criar sistemas que possam simular ou replicar a capacidade humana de raciocinar, aprender, perceber, entender e tomar decisões. A IA tem aplicações em uma variedade de setores, como assistentes virtuais, carros autônomos, diagnóstico médico e chatbots.\n",
    "\n",
    "Em resumo, Data Science é o campo de estudo que trata da extração de insights de dados, Machine Learning é uma subárea da IA que permite que os computadores aprendam com os dados, e a Inteligência Artificial é o campo mais amplo que busca criar sistemas inteligentes que possam executar tarefas que normalmente exigiriam inteligência humana.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Análise de Dados Tradicional x Ciência de Dados\n",
    "\n",
    "A análise de dados tradicional e a ciência de dados diferem em termos de abordagem, escopo e metodologia. Aqui estão algumas diferenças-chave entre essas duas abordagens:\n",
    "\n",
    "1. Escopo: A análise de dados tradicional tende a ser mais focada em responder a perguntas específicas e resolver problemas específicos usando técnicas estatísticas e ferramentas tradicionais de análise de dados. A ciência de dados, por outro lado, possui um escopo mais amplo e abrangente, concentrando-se na extração de insights e conhecimentos valiosos a partir de grandes volumes de dados estruturados e não estruturados. A ciência de dados busca descobrir padrões, tendências e relações nos dados para gerar valor e tomar decisões informadas.\n",
    "\n",
    "2. Abordagem: A análise de dados tradicional geralmente segue uma abordagem mais estruturada e sequencial, com foco em técnicas estatísticas tradicionais, como regressão linear, análise de variância e testes de hipóteses. Ela é mais orientada para a confirmação de hipóteses pré-definidas. Por outro lado, a ciência de dados adota uma abordagem mais flexível e iterativa, incorporando técnicas de machine learning, mineração de dados, visualização de dados e outras ferramentas e métodos avançados para explorar os dados, identificar padrões ocultos e criar modelos preditivos ou descritivos.\n",
    "\n",
    "3. Tecnologia e Ferramentas: A análise de dados tradicional muitas vezes usa ferramentas estatísticas tradicionais, como planilhas, bancos de dados relacionais e softwares estatísticos como SPSS ou SAS. Por outro lado, a ciência de dados faz uso extensivo de linguagens de programação, como Python e R, juntamente com bibliotecas e frameworks especializados em análise de dados, como pandas, scikit-learn e TensorFlow. Além disso, a ciência de dados também se beneficia de tecnologias de Big Data, como Hadoop e Spark, para lidar com grandes volumes de dados.\n",
    "\n",
    "4. Habilidades e Perfil dos Profissionais: A análise de dados tradicional muitas vezes é conduzida por estatísticos ou analistas de dados que possuem conhecimentos estatísticos e habilidades analíticas. A ciência de dados requer uma combinação de habilidades em programação, estatística, matemática, visualização de dados, conhecimento de negócios e curiosidade para explorar os dados. Os cientistas de dados são profissionais multidisciplinares com experiência em lidar com dados em escala e aplicar técnicas avançadas de análise.\n",
    "\n",
    "Em resumo, a análise de dados tradicional é uma abordagem mais focada e orientada a perguntas específicas, utilizando técnicas estatísticas tradicionais. Por outro lado, a ciência de dados é uma abordagem mais ampla e avançada, envolvendo técnicas de machine learning, mineração de dados e visualização de dados para descobrir insights valiosos a partir de grandes volumes de dados. A ciência de dados requer habilidades e conhecimentos adicionais em programação e estatística, além de uma mentalidade exploratória e criativa."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Graficos :\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dados de exemplo\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y1 = [1, 4, 9, 16, 25]\n",
    "y2 = [1, 8, 27, 64, 125]\n",
    "\n",
    "# Criação da figura e dos eixos\n",
    "fig, axs = plt.subplots(2, 1, figsize=(8, 6))\n",
    "\n",
    "# Plotagem do primeiro gráfico\n",
    "axs[0].plot(x, y1, 'r-', label='Gráfico 1')\n",
    "axs[0].set_xlabel('X')\n",
    "axs[0].set_ylabel('Y1')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plotagem do segundo gráfico\n",
    "axs[1].plot(x, y2, 'g-', label='Gráfico 2')\n",
    "axs[1].set_xlabel('X')\n",
    "axs[1].set_ylabel('Y2')\n",
    "axs[1].legend()\n",
    "\n",
    "# Ajuste de espaçamento entre os subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Exibição da figura\n",
    "plt.show()\n",
    "````\n",
    "ou\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dados de exemplo\n",
    "dados1 = [1, 2, 3, 4, 5]\n",
    "dados2 = [2, 4, 6, 8, 10]\n",
    "\n",
    "# Criação da figura e dos eixos\n",
    "fig, axs = plt.subplots(2, 1, figsize=(8, 6))\n",
    "\n",
    "# Plotagem do primeiro gráfico usando seaborn\n",
    "sns.lineplot(x=dados1, y=dados1, ax=axs[0])\n",
    "axs[0].set_xlabel('X')\n",
    "axs[0].set_ylabel('Y1')\n",
    "\n",
    "# Plotagem do segundo gráfico usando seaborn\n",
    "sns.lineplot(x=dados1, y=dados2, ax=axs[1])\n",
    "axs[1].set_xlabel('X')\n",
    "axs[1].set_ylabel('Y2')\n",
    "\n",
    "# Ajuste de espaçamento entre os subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Exibição da figura\n",
    "plt.show()\n",
    "\n",
    "````\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Seleções de Dados:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-19T23:32:28.859785200Z",
     "start_time": "2023-06-19T23:32:28.812759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Nome  Idade     Cidade\n",
      "2  Carlos     35  São Paulo\n",
      "3     Ana     40   Brasília\n",
      "    Nome  Idade          Cidade\n",
      "0   João     25       São Paulo\n",
      "1  Maria     30  Rio de Janeiro\n",
      "     Nome  Idade     Cidade\n",
      "0    João     25  São Paulo\n",
      "2  Carlos     35  São Paulo\n",
      "     Nome  Idade     Cidade\n",
      "2  Carlos     35  São Paulo\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Criando um DataFrame de exemplo\n",
    "data = {'Nome': ['João', 'Maria', 'Carlos', 'Ana'],\n",
    "        'Idade': [25, 30, 35, 40],\n",
    "        'Cidade': ['São Paulo', 'Rio de Janeiro', 'São Paulo', 'Brasília']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Seleção de dados com base em rótulos\n",
    "print(df.loc[df['Idade'] > 30])\n",
    "\n",
    "# Seleção de dados com base em posição\n",
    "print(df.iloc[0:2])\n",
    "\n",
    "# Seleção de dados com base em critérios\n",
    "print(df.query(\"Cidade == 'São Paulo'\"))\n",
    "\n",
    "# Filtragem com operadores lógicos\n",
    "print(df[(df['Idade'] > 30) & (df['Cidade'] == 'São Paulo')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Agregações com groupby:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Idade\n",
      "Cidade               \n",
      "Brasília         40.0\n",
      "Rio de Janeiro   30.0\n",
      "São Paulo        30.0\n",
      "                        Nome\n",
      "Cidade         Idade        \n",
      "Brasília       40        Ana\n",
      "Rio de Janeiro 30      Maria\n",
      "São Paulo      25       João\n",
      "               35     Carlos\n",
      "Cidade\n",
      "Brasília          1\n",
      "Rio de Janeiro    1\n",
      "São Paulo         2\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joaov\\AppData\\Local\\Temp\\ipykernel_14884\\1556975985.py:2: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  print(df.groupby('Cidade').mean())\n"
     ]
    }
   ],
   "source": [
    "# Agregação por grupo\n",
    "print(df.groupby('Cidade').mean())\n",
    "\n",
    "# Agregação por múltiplas colunas\n",
    "print(df.groupby(['Cidade', 'Idade']).sum())\n",
    "\n",
    "# Contagem de registros em cada grupo\n",
    "print(df.groupby('Cidade').size())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T23:32:29.041797600Z",
     "start_time": "2023-06-19T23:32:29.019759700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Junção de Dados com merge:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B   C\n",
      "0  A1  B1  C1\n"
     ]
    }
   ],
   "source": [
    "# Criando dois DataFrames de exemplo\n",
    "df1 = pd.DataFrame({'A': ['A1', 'A2', 'A3'],\n",
    "                    'B': ['B1', 'B2', 'B3']})\n",
    "df2 = pd.DataFrame({'A': ['A1', 'A4', 'A5'],\n",
    "                    'C': ['C1', 'C4', 'C5']})\n",
    "\n",
    "# Junção com base em coluna em comum (inner join)\n",
    "merged_inner = pd.merge(df1, df2, on='A', how='inner')\n",
    "print(merged_inner)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T23:32:29.271765200Z",
     "start_time": "2023-06-19T23:32:29.257765400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformação de Valores:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "df.drop(columns='b',inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Nome  Idade    Cidade\n",
      "0    João     25        SP\n",
      "1   Maria     30        RJ\n",
      "2  Carlos     35        SP\n",
      "3     Ana     40  Brasília\n",
      "     Nome  Idade    Cidade\n",
      "0    João     25        SP\n",
      "1   Maria     30        RJ\n",
      "2  Carlos     35        SP\n",
      "3     Ana     40  Brasília\n"
     ]
    }
   ],
   "source": [
    "# Substituição de valores\n",
    "df['Cidade'].replace({'São Paulo': 'SP', 'Rio de Janeiro': 'RJ'}, inplace=True)\n",
    "print(df)\n",
    "\n",
    "# Preenchimento de valores ausentes\n",
    "df['Idade'].fillna(df['Idade'].mean(), inplace=True)\n",
    "print(df)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T23:32:29.598772200Z",
     "start_time": "2023-06-19T23:32:29.585771100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Criação de Novos Valores:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "No exemplo anterior em que foi utilizado o Pandas para criar um modelo de regressão linear com a função `ols()` do pacote `statsmodels`, não foi necessário fazer o one-hot encoding.\n",
    "\n",
    "A função `ols()` já é capaz de lidar com variáveis categóricas de forma implícita. Quando uma variável categórica é incluída na fórmula do modelo usando a função `formula`, o pacote `statsmodels` automaticamente trata essa variável como uma variável categórica e realiza a codificação adequada internamente.\n",
    "\n",
    "Dessa forma, não é necessário realizar explicitamente o one-hot encoding para variáveis categóricas ao usar a função `formula` do `statsmodels`. A função trata automaticamente as variáveis categóricas, considerando-as como indicadores (dummy variables) durante a construção do modelo de regressão linear.\n",
    "\n",
    "No exemplo que mencionei, com a fórmula `Valor ~ Cor`, o `statsmodels` internamente codificará a variável categórica \"Cor\" para uso no modelo de regressão linear, sem que seja necessário fazer o one-hot encoding manualmente.\n",
    "\n",
    "Portanto, ao utilizar a função `formula` do `statsmodels`, não é necessário fazer o one-hot encoding manualmente para variáveis categóricas. A função `ols()` lida com isso de forma transparente durante a criação do modelo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Nome  Idade    Cidade  Cidade_Encoded\n",
      "0    João     25        SP               2\n",
      "1   Maria     30        RJ               1\n",
      "2  Carlos     35        SP               2\n",
      "3     Ana     40  Brasília               0\n",
      "     Nome  Idade    Cidade  Cidade_Encoded  Soma_Idade\n",
      "0    João     25        SP               2          50\n",
      "1   Maria     30        RJ               1          60\n",
      "2  Carlos     35        SP               2          70\n",
      "3     Ana     40  Brasília               0          80\n",
      "     Nome  Idade    Cidade  Cidade_Encoded  Soma_Idade  Idade_Normalizada\n",
      "0    João     25        SP               2          50           0.000000\n",
      "1   Maria     30        RJ               1          60           0.333333\n",
      "2  Carlos     35        SP               2          70           0.666667\n",
      "3     Ana     40  Brasília               0          80           1.000000\n"
     ]
    }
   ],
   "source": [
    "# Label Encoding\n",
    "df['Cidade_Encoded'] = df['Cidade'].astype('category').cat.codes\n",
    "print(df)\n",
    "\n",
    "# Criação de uma nova coluna com a soma de duas colunas existentes\n",
    "df['Soma_Idade'] = df['Idade'] + df['Idade']\n",
    "print(df)\n",
    "\n",
    "# Normalização Min-Max\n",
    "df['Idade_Normalizada'] = (df['Idade'] - df['Idade'].min()) / (df['Idade'].max() - df['Idade'].min())\n",
    "print(df)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T23:32:29.998089600Z",
     "start_time": "2023-06-19T23:32:29.949780Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Min max sklearn e pandas (NORMALIZAÇÃO):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "0. Normalização Dele\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Entradas e Saídas\n",
    "X = breast.drop(columns=['id','diagnosis'])\n",
    "X = X / X.max()\n",
    "y = breast['diagnosis']\n",
    "\n",
    "# Separando conjuntos de Treinamento e Teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=1)\n",
    "\n",
    "# Definição do modelo\n",
    "clf = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Treinamento\n",
    "clf.fit(X_train,y_train)\n",
    "```\n",
    "1. Normalização Min-Max com Scikit-learn (`sklearn`):\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Criando um array de exemplo\n",
    "data = [[1], [3], [5], [7], [9]]\n",
    "\n",
    "# Criando uma instância do MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Aplicando a normalização Min-Max aos dados\n",
    "normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "print(normalized_data)\n",
    "```\n",
    "Neste exemplo, usamos a classe `MinMaxScaler` do Scikit-learn para realizar a normalização Min-Max. Os dados são ajustados usando o método `fit_transform()` e, em seguida, os dados normalizados são impressos.\n",
    "\n",
    "2. Normalização Min-Max com Pandas:\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Criando um DataFrame de exemplo\n",
    "data = {'Coluna1': [10, 20, 30, 40, 50]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Aplicando a normalização Min-Max usando o método min-max do Pandas\n",
    "df_normalized = (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "print(df_normalized)\n",
    "```\n",
    "Neste exemplo, usamos o método `min()` e `max()` do Pandas para encontrar o valor mínimo e máximo da coluna desejada e, em seguida, aplicamos a fórmula da normalização Min-Max para normalizar os valores entre 0 e 1.\n",
    "\n",
    "3. Normalização Min-Max com Statsmodels:\n",
    "```python\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Criando um array de exemplo\n",
    "data = [[1], [3], [5], [7], [9]]\n",
    "\n",
    "# Criando uma instância do MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Aplicando a normalização Min-Max aos dados\n",
    "normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "# Usando o modelo de regressão linear do Statsmodels com os dados normalizados\n",
    "X_normalized = sm.add_constant(normalized_data)\n",
    "y = [2, 4, 6, 8, 10]\n",
    "\n",
    "model = sm.OLS(y, X_normalized)\n",
    "results = model.fit()\n",
    "\n",
    "print(results.summary())\n",
    "```\n",
    "Neste exemplo, usamos o `MinMaxScaler` do Scikit-learn para normalizar os dados. Em seguida, adicionamos a constante usando `sm.add_constant()` do Statsmodels e ajustamos um modelo de regressão linear usando o `OLS` do Statsmodels.\n",
    "\n",
    "Esses exemplos mostram como realizar a normalização Min-Max utilizando diferentes bibliotecas e suas respectivas funções. Cada abordagem pode ser escolhida com base na preferência e necessidades específicas do seu projeto."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Cor  Amarelo  Azul  Verde  Vermelho\n",
      "0  Vermelho        0     0      0         1\n",
      "1      Azul        0     1      0         0\n",
      "2     Verde        0     0      1         0\n",
      "3  Vermelho        0     0      0         1\n",
      "4   Amarelo        1     0      0         0\n"
     ]
    }
   ],
   "source": [
    "#Hot encode\n",
    "import pandas as pd\n",
    "\n",
    "# Criando um DataFrame de exemplo\n",
    "data = {'Cor': ['Vermelho', 'Azul', 'Verde', 'Vermelho', 'Amarelo']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Realizando o one-hot encoding\n",
    "one_hot_encoded = pd.get_dummies(df['Cor'])\n",
    "df_encoded = pd.concat([df, one_hot_encoded], axis=1)\n",
    "\n",
    "print(df_encoded)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T23:32:30.357824Z",
     "start_time": "2023-06-19T23:32:30.308790100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "O Label Encoding e o One-Hot Encoding são técnicas de codificação utilizadas para lidar com variáveis categóricas em tarefas de análise de dados e modelagem. A principal diferença entre eles está na forma como eles representam as categorias.\n",
    "\n",
    "1. Label Encoding:\n",
    "O Label Encoding é uma técnica de codificação que atribui um número inteiro único a cada categoria da variável categórica. Cada categoria é mapeada para um número, geralmente começando do zero. Por exemplo:\n",
    "\n",
    "```\n",
    "Categorias: A, B, C, A, B\n",
    "\n",
    "Label Encoding: 0, 1, 2, 0, 1\n",
    "```\n",
    "\n",
    "Nesse caso, cada categoria é representada por um número inteiro. O Label Encoding é útil quando há uma ordem intrínseca nas categorias, pois a representação numérica preserva essa ordem. No entanto, o Label Encoding pode levar a problemas quando aplicado a variáveis sem ordem ou quando o modelo interpreta erroneamente a codificação como uma relação ordinal.\n",
    "\n",
    "2. One-Hot Encoding:\n",
    "O One-Hot Encoding é uma técnica de codificação que cria colunas binárias separadas para cada categoria da variável categórica. Cada coluna representa uma categoria, e um valor de 1 indica a presença da categoria, enquanto um valor de 0 indica a ausência. Por exemplo:\n",
    "\n",
    "```\n",
    "Categorias: A, B, C, A, B\n",
    "\n",
    "One-Hot Encoding:\n",
    "  A  B  C\n",
    "0  1  0  0\n",
    "1  0  1  0\n",
    "2  0  0  1\n",
    "3  1  0  0\n",
    "4  0   1  0\n",
    "```\n",
    "\n",
    "Nesse caso, três colunas são criadas, uma para cada categoria única. Cada linha indica a presença ou ausência da categoria correspondente. O One-Hot Encoding é útil quando não há uma ordem específica nas categorias e quando não é necessário manter a relação ordinal entre elas.\n",
    "\n",
    "Em resumo, o Label Encoding atribui um número inteiro único a cada categoria, enquanto o One-Hot Encoding cria colunas binárias separadas para cada categoria. A escolha entre eles depende da natureza dos dados, da existência ou não de uma ordem intrínseca nas categorias e dos requisitos do modelo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### predição com hot encode Pandas (hotencode no statsmodel é automatico)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  Valor   R-squared:                       0.977\n",
      "Model:                            OLS   Adj. R-squared:                  0.909\n",
      "Method:                 Least Squares   F-statistic:                     14.33\n",
      "Date:                Mon, 19 Jun 2023   Prob (F-statistic):              0.191\n",
      "Time:                        20:32:30   Log-Likelihood:                -4.8040\n",
      "No. Observations:                   5   AIC:                             17.61\n",
      "Df Residuals:                       1   BIC:                             16.05\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===================================================================================\n",
      "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "Intercept          12.0000      1.414      8.485      0.075      -5.969      29.969\n",
      "Cor[T.Azul]         8.0000      2.000      4.000      0.156     -17.412      33.412\n",
      "Cor[T.Verde]        3.0000      2.000      1.500      0.374     -22.412      28.412\n",
      "Cor[T.Vermelho]    -3.0000      1.732     -1.732      0.333     -25.008      19.008\n",
      "==============================================================================\n",
      "Omnibus:                          nan   Durbin-Watson:                   1.500\n",
      "Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.052\n",
      "Skew:                           0.000   Prob(JB):                        0.974\n",
      "Kurtosis:                       2.500   Cond. No.                         5.42\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joaov\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\stats\\stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 5 samples were given.\n",
      "  warn(\"omni_normtest is not valid with less than 8 observations; %i \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Criando um DataFrame de exemplo\n",
    "data = {'Cor': ['Vermelho', 'Azul', 'Verde', 'Vermelho', 'Amarelo'],\n",
    "        'Valor': [10, 20, 15, 8, 12]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Criando o modelo de regressão linear usando a função formula\n",
    "model = smf.ols(formula='Valor ~ Cor', data=df)\n",
    "# lm = sm.ols(formula='CO2EMISSIONS ~ FUELCONSUMPTION_COMB + ENGINESIZE + FUELTYPE', data=df)\n",
    "# Ajustando o modelo aos dados\n",
    "results = model.fit()\n",
    "\n",
    "print(results.summary())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T23:32:30.703789900Z",
     "start_time": "2023-06-19T23:32:30.666792400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  Valor   R-squared:                       0.977\n",
      "Model:                            OLS   Adj. R-squared:                  0.909\n",
      "Method:                 Least Squares   F-statistic:                     14.33\n",
      "Date:                Mon, 19 Jun 2023   Prob (F-statistic):              0.191\n",
      "Time:                        20:32:30   Log-Likelihood:                -4.8040\n",
      "No. Observations:                   5   AIC:                             17.61\n",
      "Df Residuals:                       1   BIC:                             16.05\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         12.0000      1.414      8.485      0.075      -5.969      29.969\n",
      "Azul           8.0000      2.000      4.000      0.156     -17.412      33.412\n",
      "Verde          3.0000      2.000      1.500      0.374     -22.412      28.412\n",
      "Vermelho      -3.0000      1.732     -1.732      0.333     -25.008      19.008\n",
      "==============================================================================\n",
      "Omnibus:                          nan   Durbin-Watson:                   1.500\n",
      "Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.052\n",
      "Skew:                           0.000   Prob(JB):                        0.974\n",
      "Kurtosis:                       2.500   Cond. No.                         5.42\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joaov\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\stats\\stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 5 samples were given.\n",
      "  warn(\"omni_normtest is not valid with less than 8 observations; %i \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Criando um DataFrame de exemplo\n",
    "data = {'Cor': ['Vermelho', 'Azul', 'Verde', 'Vermelho', 'Amarelo'],\n",
    "        'Valor': [10, 20, 15, 8, 12]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Realizando o one-hot encoding\n",
    "one_hot_encoded = pd.get_dummies(df['Cor'], drop_first=True)\n",
    "df_encoded = pd.concat([df['Valor'], one_hot_encoded], axis=1)\n",
    "\n",
    "# Adicionando uma coluna de interceptação ao DataFrame\n",
    "df_encoded = sm.add_constant(df_encoded)\n",
    "\n",
    "# Criando o modelo de regressão linear\n",
    "model = sm.OLS(df_encoded['Valor'], df_encoded.drop('Valor', axis=1))\n",
    "\n",
    "# Ajustando o modelo aos dados\n",
    "results = model.fit()\n",
    "\n",
    "print(results.summary())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T23:32:30.854834600Z",
     "start_time": "2023-06-19T23:32:30.786794500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "A regressão linear do Scikit-learn espera que todas as variáveis de entrada sejam numéricas. Portanto, é necessário converter as variáveis categóricas em representações numéricas antes de ajustar o modelo.\n",
    "\n",
    "- Se você optar por usar o Label Encoding, deve-se aplicá-lo às categorias e substituí-las pelos valores numéricos correspondentes.\n",
    "\n",
    "- Se preferir usar o One-Hot Encoding, deve-se criar colunas binárias separadas para cada categoria, representando a presença ou ausência de cada categoria nos dados.\n",
    "\n",
    "Aqui está um exemplo de como realizar o Label Encoding e o One-Hot Encoding usando o Scikit-learn antes de ajustar um modelo de regressão linear:\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### predição com hot encode Sklearn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9. 20. 15.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joaov\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Criando um DataFrame de exemplo\n",
    "data = {'Cor': ['Vermelho', 'Azul', 'Verde', 'Vermelho', 'Amarelo'],\n",
    "        'Valor': [10, 20, 15, 8, 12]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Realizando o Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df['Cor_encoded'] = label_encoder.fit_transform(df['Cor'])\n",
    "\n",
    "# Realizando o One-Hot Encoding\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "onehot_encoded = onehot_encoder.fit_transform(df[['Cor']])\n",
    "\n",
    "# Ajustando o modelo de regressão linear aos dados\n",
    "X = onehot_encoded  # Usando One-Hot Encoding\n",
    "# X = df[['Cor_encoded']]  # Usando Label Encoding\n",
    "y = df['Valor']\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Realizando previsões com o modelo treinado\n",
    "new_data = {'Cor': ['Vermelho', 'Azul', 'Verde']}\n",
    "df_new = pd.DataFrame(new_data)\n",
    "\n",
    "# Aplicando o mesmo Label Encoding/One-Hot Encoding aos novos dados\n",
    "df_new['Cor_encoded'] = label_encoder.transform(df_new['Cor'])\n",
    "new_onehot_encoded = onehot_encoder.transform(df_new[['Cor']])\n",
    "\n",
    "# Fazendo as previsões com o modelo treinado\n",
    "predictions = model.predict(new_onehot_encoded)\n",
    "\n",
    "print(predictions)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T23:32:31.215841300Z",
     "start_time": "2023-06-19T23:32:31.162799900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Neste exemplo, o Label Encoding é realizado usando a classe `LabelEncoder` do Scikit-learn para transformar as categorias em valores numéricos. Já o One-Hot Encoding é realizado usando a classe `OneHotEncoder` para criar as colunas binárias separadas.\n",
    "\n",
    "Observe que é necessário aplicar o mesmo processo de codificação aos novos dados antes de fazer previsões com o modelo treinado.\n",
    "\n",
    "Portanto, ao usar a regressão linear do Scikit-learn, é necessário aplicar o one-hot encoding ou label encoding manualmente às variáveis categóricas antes de ajustar o modelo. A escolha entre as duas técnicas depende da natureza dos dados e dos requisitos do modelo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Visualização de Dados, Matplotlib, Seaborn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " Aqui estão exemplos de comandos para criar gráficos de evolução, distribuição, proporções e tendências usando a camada \"artist layer\" nas bibliotecas Seaborn e Matplotlib:\n",
    "\n",
    "1. Evolução (Gráfico de Linha):\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Criando a figura e os eixos manualmente\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plotando um gráfico de linha\n",
    "ax.plot(x, y)\n",
    "\n",
    "# Personalizando os elementos do gráfico\n",
    "ax.set_xlabel('Tempo')\n",
    "ax.set_ylabel('Valores')\n",
    "ax.set_title('Gráfico de Evolução')\n",
    "\n",
    "# Mostrando o gráfico\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "2. Distribuição (Gráfico de Histograma):\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Criando a figura e os eixos manualmente\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plotando um gráfico de histograma\n",
    "sns.histplot(data, ax=ax)\n",
    "\n",
    "# Personalizando os elementos do gráfico\n",
    "ax.set_xlabel('Valores')\n",
    "ax.set_ylabel('Frequência')\n",
    "ax.set_title('Gráfico de Distribuição')\n",
    "\n",
    "# Mostrando o gráfico\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "3. Proporções (Gráfico de Pizza):\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Criando a figura e os eixos manualmente\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plotando um gráfico de pizza\n",
    "ax.pie(sizes, labels=labels, autopct='%1.1f%%')\n",
    "\n",
    "# Personalizando os elementos do gráfico\n",
    "ax.set_title('Gráfico de Proporções')\n",
    "\n",
    "# Mostrando o gráfico\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "4. Tendências (Gráfico de Dispersão):\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Criando a figura e os eixos manualmente\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plotando um gráfico de dispersão\n",
    "sns.scatterplot(x='x', y='y', data=df, ax=ax)\n",
    "\n",
    "# Personalizando os elementos do gráfico\n",
    "ax.set_xlabel('Variável X')\n",
    "ax.set_ylabel('Variável Y')\n",
    "ax.set_title('Gráfico de Tendências')\n",
    "\n",
    "# Mostrando o gráfico\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Nesses exemplos, você cria manualmente a figura (`fig`) e os eixos (`ax`) usando a função `subplots()` do Matplotlib. Em seguida, você utiliza os métodos disponíveis no objeto `ax` para plotar os gráficos e personalizar os elementos visuais.\n",
    "\n",
    "Lembre-se de importar as bibliotecas Seaborn e Matplotlib antes de executar os comandos acima. Esses são apenas exemplos básicos para demonstrar a criação de diferentes tipos de gráficos usando a camada \"artist layer\". Você pode explorar ainda mais os métodos e atributos disponíveis nos objetos `ax` para personalizar seus gráficos de acordo com suas necessidades."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Densidade probabilidade\n",
    "Aqui estão exemplos de comandos para criar gráficos de densidade e probabilidade usando a camada \"artist layer\" nas bibliotecas Seaborn e Matplotlib:\n",
    "\n",
    "1. Gráfico de Densidade (KDE - Estimativa de Densidade Kernel):\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Criando a figura e os eixos manualmente\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plotando um gráfico de densidade\n",
    "sns.kdeplot(data, ax=ax)\n",
    "\n",
    "# Personalizando os elementos do gráfico\n",
    "ax.set_xlabel('Valores')\n",
    "ax.set_ylabel('Densidade')\n",
    "ax.set_title('Gráfico de Densidade')\n",
    "\n",
    "# Mostrando o gráfico\n",
    "plt.show()\n",
    "```\n",
    "1.1 Gerar histograma e probabilidade :\n",
    "```python\n",
    "sns.histplot(x='body_mass_g', hue='sex', data=df_penguins, kde=True)\n",
    "````\n",
    "2. Gráfico de Probabilidade (QQ Plot):\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Criando a figura e os eixos manualmente\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plotando um gráfico de probabilidade\n",
    "sns.probplot(data, plot=ax)\n",
    "\n",
    "# Personalizando os elementos do gráfico\n",
    "ax.set_xlabel('Quantis Teóricos')\n",
    "ax.set_ylabel('Valores Observados')\n",
    "ax.set_title('Gráfico de Probabilidade')\n",
    "\n",
    "# Mostrando o gráfico\n",
    "plt.show()\n",
    "```\n",
    "3. Desnsidade probabilidade acomulada:\n",
    "```python\n",
    "sns.ecdfplot(data=df, x='dis')\n",
    "plt.title('DIS', fontsize=14, weight='bold')\n",
    "plt.xlabel('DIS')\n",
    "plt.ylabel('Probabilidade ACOMULADA')\n",
    "```\n",
    "4. Pegar o valor exato da densidade :\n",
    "```python\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Parâmetros da distribuição normal\n",
    "media = 0\n",
    "desvio_padrao = 1\n",
    "\n",
    "# Valor específico em x\n",
    "x = 1.5\n",
    "\n",
    "# Obtendo a densidade de probabilidade (pdf) e densidade acumulada (cdf)\n",
    "densidade = norm.pdf(x, loc=media, scale=desvio_padrao)\n",
    "densidade_acumulada = norm.cdf(x, loc=media, scale=desvio_padrao)\n",
    "\n",
    "print(\"Densidade de Probabilidade:\", densidade)\n",
    "print(\"Densidade Acumulada:\", densidade_acumulada)\n",
    "```\n",
    "Nesses exemplos, você cria manualmente a figura (`fig`) e os eixos (`ax`) usando a função `subplots()` do Matplotlib. Em seguida, você utiliza os métodos disponíveis no objeto `ax` para plotar os gráficos de densidade e probabilidade.\n",
    "\n",
    "No primeiro exemplo, usamos a função `kdeplot` do Seaborn para plotar um gráfico de densidade usando a estimativa de densidade kernel (KDE). No segundo exemplo, usamos a função `probplot` do Seaborn para plotar um gráfico de probabilidade (QQ plot), que compara os quantis teóricos de uma distribuição com os valores observados.\n",
    "\n",
    "Lembre-se de importar as bibliotecas Seaborn e Matplotlib antes de executar os comandos acima. Esses são apenas exemplos básicos para demonstrar a criação de gráficos de densidade e probabilidade usando a camada \"artist layer\". Você pode explorar ainda mais os métodos e atributos disponíveis nos objetos `ax` para personalizar seus gráficos de acordo com suas necessidades."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### BoxPlot\n",
    "Certamente! Aqui está um exemplo de comando para criar um box plot e exibir outliers usando a biblioteca Seaborn:\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Criando a figura e os eixos manualmente\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plotando um box plot com exibição de outliers\n",
    "sns.boxplot(x='variavel_x', y='variavel_y', data=df, ax=ax, showfliers=True)\n",
    "\n",
    "# Personalizando os elementos do gráfico\n",
    "ax.set_xlabel('Variável X')\n",
    "ax.set_ylabel('Variável Y')\n",
    "ax.set_title('Box Plot com Exibição de Outliers')\n",
    "\n",
    "# Mostrando o gráfico\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Neste exemplo, você cria manualmente a figura (`fig`) e os eixos (`ax`) usando a função `subplots()` do Matplotlib. Em seguida, utiliza a função `boxplot` do Seaborn para criar o box plot com a exibição de outliers. O argumento `showfliers=True` é usado para indicar que os outliers devem ser exibidos no gráfico.\n",
    "\n",
    "Certifique-se de substituir `'variavel_x'` e `'variavel_y'` pelos nomes das variáveis que deseja plotar. Além disso, substitua `df` pelo seu DataFrame contendo os dados apropriados.\n",
    "\n",
    "Lembre-se de importar as bibliotecas Seaborn e Matplotlib antes de executar o comando acima. Este é apenas um exemplo básico para demonstrar a criação de um box plot com a exibição de outliers usando a camada \"artist layer\". Você pode explorar ainda mais os métodos e atributos disponíveis nos objetos `ax` para personalizar seu gráfico de acordo com suas necessidades."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Distribuição normal:\n",
    "Quando dizemos que um gráfico ou conjunto de dados segue uma distribuição normal, significa que os valores estão distribuídos de acordo com a chamada distribuição normal ou gaussiana. A distribuição normal é uma das distribuições mais importantes e amplamente utilizadas na estatística.\n",
    "\n",
    "Características da distribuição normal:\n",
    "\n",
    "1. Forma de sino: A distribuição normal é simétrica e possui uma forma de sino. Ela é caracterizada por ter uma média (valor central) e uma dispersão em torno dessa média.\n",
    "\n",
    "2. Média e mediana iguais: A média e a mediana dos dados distribuídos normalmente são iguais.\n",
    "\n",
    "3. Valores extremos raros: A distribuição normal tem caudas que tendem a se aproximar do eixo horizontal, o que significa que valores extremos são raros.\n",
    "\n",
    "4. Teorema do Limite Central: A distribuição normal surge naturalmente em muitos fenômenos aleatórios devido ao Teorema do Limite Central. Esse teorema afirma que a soma de um grande número de variáveis aleatórias independentes, independentemente da sua distribuição original, tenderá a se aproximar de uma distribuição normal.\n",
    "\n",
    "A distribuição normal é completamente definida pelos seus dois parâmetros principais: a média (μ) e o desvio padrão (σ). A forma da distribuição é determinada por esses parâmetros, e é possível calcular probabilidades e realizar inferências estatísticas com base nesses valores.\n",
    "\n",
    "A verificação de normalidade de um conjunto de dados pode ser feita de várias maneiras, incluindo a inspeção visual dos gráficos de histograma, gráfico de probabilidade normal (QQ plot) ou o uso de testes estatísticos como o teste de normalidade de Shapiro-Wilk ou teste de Kolmogorov-Smirnov.\n",
    "\n",
    "A suposição de normalidade é comum em muitas técnicas estatísticas, como testes de hipóteses, intervalos de confiança e modelagem estatística. No entanto, é importante ressaltar que nem todos os conjuntos de dados seguem uma distribuição normal e que existem outras distribuições estatísticas que podem ser mais adequadas para descrever um conjunto de dados específico."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Verificar se a distribuição se aproxima da normal:\n",
    "##### (boxplot deve estar com media centralizada e quartis de tamanhos iguais)\n",
    "```python\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.subplot(5,2,1)\n",
    "plt.boxplot(df.crim, vert=False)\n",
    "plt.title('CRIM', fontsize=14, weight='bold')\n",
    "plt.xlabel('CRIM')\n",
    "plt.ylabel('Probabilidade')\n",
    "````\n",
    "ou\n",
    "\n",
    "```python\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Dados para teste\n",
    "dados = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Realizando o teste de normalidade de Shapiro-Wilk\n",
    "stat, p_valor = shapiro(dados)\n",
    "\n",
    "# Verificando o resultado\n",
    "alpha = 0.05  # Nível de significância\n",
    "if p_valor > alpha:\n",
    "    print(\"Os dados provavelmente seguem uma distribuição normal\")\n",
    "else:\n",
    "    print(\"Os dados não seguem uma distribuição normal\")\n",
    "````\n",
    "ou\n",
    "\n",
    "O `sm.qqplot` é um método do pacote `statsmodels` que gera um gráfico chamado QQ plot (Quantile-Quantile plot) ou gráfico de probabilidade normal. Esse gráfico é usado para avaliar a adequação de uma distribuição teórica aos dados observados.\n",
    "\n",
    "Ao plotar um QQ plot, os valores dos dados são ordenados e comparados com os quantis correspondentes de uma distribuição de referência, geralmente uma distribuição normal. Os quantis observados são plotados no eixo vertical (eixo dos dados) e os quantis teóricos esperados são plotados no eixo horizontal (eixo da distribuição de referência). Se os dados seguirem a distribuição de referência, os pontos no gráfico seguirão uma linha reta, indicando uma boa adequação.\n",
    "\n",
    "Aqui está um exemplo de como usar `sm.qqplot` para gerar um QQ plot:\n",
    "\n",
    "```python\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dados de exemplo\n",
    "dados = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Gerando o QQ plot\n",
    "sm.qqplot(dados, line='s')\n",
    "\n",
    "# Personalizando o gráfico\n",
    "plt.title('QQ Plot')\n",
    "plt.xlabel('Quantis teóricos')\n",
    "plt.ylabel('Quantis observados')\n",
    "\n",
    "# Mostrando o gráfico\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Nesse exemplo, passamos os dados de exemplo para a função `qqplot` do `statsmodels`. O argumento `line='s'` é usado para traçar uma linha diagonal de referência. Essa linha representa uma distribuição perfeitamente normal.\n",
    "\n",
    "Ao executar o código, o QQ plot será gerado, mostrando os quantis teóricos esperados (eixo horizontal) em comparação com os quantis observados dos dados (eixo vertical). Se os pontos no gráfico seguirem aproximadamente a linha de referência, isso sugere uma boa adequação dos dados à distribuição normal. Caso contrário, desvios da linha reta podem indicar desvios da normalidade nos dados.\n",
    "\n",
    "Distribuições contínuas:\n",
    "\n",
    "Normal: stats.norm\n",
    "Exponencial: stats.expon\n",
    "Gama: stats.gamma\n",
    "Lognormal: stats.lognorm\n",
    "Weibull: stats.weibull\n",
    "Beta: stats.beta\n",
    "Cauchy: stats.cauchy\n",
    "T-Student: stats.t\n",
    "F: stats.f\n",
    "Distribuições discretas:\n",
    "\n",
    "Bernoulli: stats.bernoulli\n",
    "Binomial: stats.binom\n",
    "Poisson: stats.poisson\n",
    "Geométrica: stats.geom\n",
    "Hipergeométrica: stats.hypergeom\n",
    "Negative Binomial: stats.nbinom"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Linhas de tendencia:\n",
    "```Python\n",
    "coefs = np.polyfit(x, y, deg=1)\n",
    "tendencia = np.poly1d(coefs)\n",
    "plt.plot(x, tendencia(x), color='red')\n",
    "````"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  Teste de hipótese\n",
    "\n",
    "````python\n",
    "# Aplica o teste de hipótese t-student para comparar o peso dos pinguins machos e fêmeas\n",
    "tstat, pval = ttest_ind(df_male['body_mass_g'], df_female['body_mass_g'])\n",
    "\n",
    "print('O valor de t-estatística é:', tstat)\n",
    "print('O valor de p-valor é:', pval)\n",
    "````\n",
    "\n",
    "Quanto menor o valor de p-valor, mais forte é a evidência contra a hipótese nula. < 0.05\n",
    "Quanto maior o valor absoluto da t-estatística, mais improvável é que a diferença observada seja devida ao acaso e mais forte é a evidência contra a hipótese nula."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Regressoes:\n",
    "\n",
    "Separando conjuntos de Treinamento e Teste:\n",
    "X = breast.drop(columns=['id','diagnosis'])\n",
    "X = X / X.max()\n",
    "y = breast['diagnosis']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Linear"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as sm\n",
    "```\n",
    "##### Regressao Simples com statsmodel:\n",
    "```python\n",
    "# Definição do modelo\n",
    "lm = sm.ols(formula='CO2EMISSIONS ~ FUELCONSUMPTION_COMB', data=df)\n",
    "\n",
    "# Treinamento\n",
    "lm = lm.fit()\n",
    "\n",
    "# Resultados\n",
    "print(lm.summary())\n",
    "\n",
    "print(f\"R2 = {lm.rsquared}\")\n",
    "```\n",
    "Previsão:\n",
    "```python\n",
    "valores_previsao = [4, 28]\n",
    "\n",
    "for valor in valores_previsao:\n",
    "  emissao_co2 = lm.predict(pd.DataFrame({'FUELCONSUMPTION_COMB': [valor]}))[0]\n",
    "  print(f\"Consumo de Combustível = {valor} -> Emissão de CO2 = {emissao_co2:.3f}\")\n",
    "```\n",
    "Regressao  multipla\n",
    "````python\n",
    "# Definição do modelo\n",
    "lm = sm.ols(formula='CO2EMISSIONS ~ FUELCONSUMPTION_COMB + ENGINESIZE ', data=df)\n",
    "\n",
    "# Treinamento\n",
    "lm = lm.fit()\n",
    "\n",
    "# Resultados\n",
    "print(lm.summary())\n",
    "````\n",
    "\n",
    "##### Regressao simples com sklearn e normalização:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Entradas e Saídas\n",
    "X = breast.drop(columns=['id','diagnosis'])\n",
    "X = X / X.max()\n",
    "y = breast['diagnosis']\n",
    "\n",
    "# Definição\n",
    "clf = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Treinamento\n",
    "clf.fit(X,y)\n",
    "\n",
    "print(clf.score(X,y))\n",
    "````\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Regressao Logistica:\n",
    "\n",
    "SKLEARN:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "````python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = LogisticRegression(max_iter=10000)\n",
    "clf.fit(x,y)\n",
    "'''print( clf.coef_, clf.intercept_)\n",
    "coeficientes = clf.coef_\n",
    "# Contar o número de coeficientes positivos\n",
    "# Os coeficientes positivos indicam uma relação positiva entre o atributo e a classe benigna do tumor.\n",
    "num_atributos_positivos = sum(coef > 0 for coef in coeficientes[0])\n",
    "num_atributos_negativos = sum(coef < 0 for coef in coeficientes[0])\n",
    "print(\"Número de atributos positivamente relacionados com o aspecto benigno:\", num_atributos_positivos)\n",
    "print(\"Número de atributos negativamente relacionados com o aspecto benigno:\", num_atributos_negativos)\n",
    "'''\n",
    "# Acuracidade\n",
    "# Avaliação\n",
    "y_pred = clf.predict(X)\n",
    "print(clf.score(X,y))\n",
    "print(accuracy_score(y, y_pred))\n",
    "print(np.unique(y_pred,return_counts=True)) # quantos casos benignos e malignos\n",
    "````\n",
    "\n",
    "ou\n",
    "\n",
    "com normalização:\n",
    "````python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Entradas e Saídas\n",
    "X = breast.drop(columns=['id','diagnosis'])\n",
    "X = X / X.max()\n",
    "y = breast['diagnosis']\n",
    "\n",
    "# Definição\n",
    "clf = LinearRegression()\n",
    "\n",
    "# Treinamento\n",
    "clf.fit(X,y)\n",
    "\n",
    "print(clf.score(X,y))\n",
    "\n",
    "# Avaliação\n",
    "y_pred = clf.predict(X) > 0.5\n",
    "\n",
    "# Acuracidade\n",
    "print(sum(y_pred == y) / len(y))\n",
    "````\n",
    "\n",
    "ou\n",
    "com separação entre conjunto de teste e treinamento:\n",
    "````python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Entradas e Saídas\n",
    "X = breast.drop(columns=['id','diagnosis'])\n",
    "X = X / X.max()\n",
    "y = breast['diagnosis']\n",
    "\n",
    "# Separando conjuntos de Treinamento e Teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=1)\n",
    "\n",
    "# Definição do modelo\n",
    "clf = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Treinamento\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "# Avaliação\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('\\nMatriz de Confusão:\\n')\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "cm_df = pd.DataFrame(cm,index=[clf.classes_],columns=[clf.classes_])\n",
    "display(cm_df)\n",
    "print()\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('\\nScore de Acuracidade (1):\\n')\n",
    "print(f'{accuracy:.4f}')\n",
    "\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print('\\nScore de Acuracidade (2):\\n')\n",
    "print(f'{accuracy:.4f}')\n",
    "\n",
    "print('\\nClassification Report:\\n')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# Novos casos\n",
    "X_new = new_breast.drop(columns=['id','diagnosis'])\n",
    "X_new = X_new / breast.drop(columns=['id','diagnosis']).max()\n",
    "y_pred = clf.predict(X_new)\n",
    "\n",
    "new_breast.diagnosis = y_pred\n",
    "\n",
    "new_breast.diagnosis.value_counts()\n",
    "\n",
    "````"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![imagem](https://github.com/Rogerio-mack/Machine-Learning-I/raw/main/Figures/matriz_de_confusao.png)\n",
    "\n",
    "A precisão (precision), o recall (recall) e o F1-score (F1 score) são métricas utilizadas para avaliar o desempenho de um modelo de classificação em um problema de aprendizado de máquina. Cada uma dessas métricas desempenha um papel importante na avaliação do desempenho do modelo em diferentes aspectos.\n",
    "\n",
    "A precisão é a métrica que mede a proporção de verdadeiros positivos em relação à soma dos verdadeiros positivos e falsos positivos. Em outras palavras, a precisão mede a capacidade do modelo de classificar corretamente as instâncias positivas em relação às instâncias que foram classificadas como positivas pelo modelo. Uma alta precisão indica uma baixa taxa de falsos positivos, ou seja, o modelo tem uma boa capacidade de identificar corretamente as instâncias positivas.\n",
    "\n",
    "O recall, também conhecido como taxa de verdadeiros positivos ou sensibilidade, mede a proporção de verdadeiros positivos em relação à soma dos verdadeiros positivos e falsos negativos. O recall avalia a capacidade do modelo de encontrar corretamente todas as instâncias positivas. Uma alta taxa de recall indica que o modelo está encontrando a maioria das instâncias positivas, minimizando os falsos negativos.\n",
    "\n",
    "O F1-score é uma medida de média harmônica entre a precisão e o recall. Ele fornece um equilíbrio entre essas duas métricas e é particularmente útil quando há um desequilíbrio entre as classes do problema. O F1-score é calculado como a média harmônica da precisão e do recall e é uma métrica que varia de 0 a 1. Um valor de F1-score próximo de 1 indica um bom equilíbrio entre precisão e recall.\n",
    "\n",
    "Em resumo, cada métrica tem seu papel específico na avaliação do desempenho do modelo. A precisão mede a proporção de verdadeiros positivos entre os classificados como positivos, o recall mede a proporção de verdadeiros positivos encontrados e o F1-score fornece um equilíbrio entre a precisão e o recall. A escolha da métrica a ser utilizada depende do contexto do problema e das necessidades específicas de avaliação."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### KNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "````python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Entradas e Saídas\n",
    "# X = breast.drop(columns=['id','diagnosis'])\n",
    "# X = X / X.max()\n",
    "# y = breast['diagnosis']\n",
    "\n",
    "# Separando conjuntos de Treinamento e Teste\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=1)\n",
    "\n",
    "# Definição do modelo\n",
    "clf = KNeighborsClassifier(n_neighbors = 3)\n",
    "\n",
    "# Treinamento\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "# Avaliação\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(clf)\n",
    "print()\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('\\nMatriz de Confusão:\\n')\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "cm_df = pd.DataFrame(cm,index=[clf.classes_],columns=[clf.classes_])\n",
    "display(cm_df)\n",
    "print()\n",
    "\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print('\\nScore de Acuracidade:\\n')\n",
    "print(f'{accuracy:.4f}')\n",
    "\n",
    "print('\\nClassification Report:\\n')\n",
    "print(classification_report(y_test, y_pred))\n",
    "````"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
